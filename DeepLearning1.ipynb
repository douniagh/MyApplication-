{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "cell_execution_strategy": "setup",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/douniagh/MyApplication-/blob/main/DeepLearning1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZRSiDYNQLSr"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import cv2\n",
        "import math\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import shutil\n",
        "from sklearn.preprocessing import QuantileTransformer\n",
        "from PIL import Image\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "import os\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import keras\n",
        "from keras import backend as K\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Input, Dense, Conv1D, Dropout, MaxPool1D, Flatten\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "-nC4T4tnIrRM"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# cnn model\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from numpy import dstack\n",
        "from pandas import read_csv\n",
        "from matplotlib import pyplot\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dropout\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.convolutional import MaxPooling1D\n",
        "from keras.utils import to_categorical"
      ],
      "metadata": {
        "id": "g00FY4tyb3lK"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Replace 'your_train_data.csv' and 'your_test_data.csv' with the filenames of your train and test data files\n",
        "# Replace the appropriate paths if the files are in a different directory\n",
        "train_file = '/content/drive/MyDrive/DataSplits/train_data.csv'\n",
        "test_file = '/content/drive/MyDrive/DataSplits/test_data.csv'\n",
        "\n",
        "# Load train data from CSV\n",
        "train_df = pd.read_csv(train_file)\n",
        "\n",
        "# Extract features and labels from train data\n",
        "X_train = train_df.drop(columns=['Attack_type']).values\n",
        "y_train = train_df['Attack_type'].values\n",
        "\n",
        "# Load test data from CSV\n",
        "test_df = pd.read_csv(test_file)\n",
        "\n",
        "# Extract features and labels from test data\n",
        "X_test = test_df.drop(columns=['Attack_type']).values\n",
        "y_test = test_df['Attack_type'].values\n",
        "\n",
        "# Print the shapes of the train and test data to verify\n",
        "print(\"X_train shape:\", X_train.shape)\n",
        "print(\"y_train shape:\", y_train.shape)\n",
        "print(\"X_test shape:\", X_test.shape)\n",
        "print(\"y_test shape:\", y_test.shape)\n"
      ],
      "metadata": {
        "id": "pWaWK8-sCM2y",
        "outputId": "bba355eb-41d8-4f6f-ff2d-72990f91dc9f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train shape: (15834598, 40)\n",
            "y_train shape: (15834598,)\n",
            "X_test shape: (3958650, 40)\n",
            "y_test shape: (3958650,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Assuming n_timesteps and n_features are pre-defined based on your dataset\n",
        "n_timesteps = 1\n",
        "n_features = 40\n",
        "\n",
        "# Reshape X_train and X_test\n",
        "X_train_reshaped = np.reshape(X_train, (X_train.shape[0], n_timesteps, n_features))\n",
        "X_test_reshaped = np.reshape(X_test, (X_test.shape[0], n_timesteps, n_features))\n",
        "\n",
        "# Print the new shapes of X_train and X_test to verify\n",
        "print(\"X_train shape:\", X_train_reshaped.shape)\n",
        "print(\"X_test shape:\", X_test_reshaped.shape)"
      ],
      "metadata": {
        "id": "S6Gl_3qUKBBy",
        "outputId": "ed8c7ecc-eb75-4705-a89a-3beb4fe4eb9d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train shape: (15834598, 1, 40)\n",
            "X_test shape: (3958650, 1, 40)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
        "\n",
        "# Assuming you have already reshaped your data as mentioned before\n",
        "# X_train shape: (19793248, 40, 1)\n",
        "# X_test shape: (3958650, 40, 1)\n",
        "\n",
        "# Build the 1D CNN model\n",
        "model = Sequential()\n",
        "\n",
        "# Add layers to the model\n",
        "model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(15834598,15834598, 40)))\n",
        "model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(100, activation='relu'))\n",
        "model.add(Dense(15, activation='softmax'))\n",
        "\n",
        "# Print the summary of the model\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "RrNGv10vGQI9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2UtCwzAjHiuZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "SJeWlljpGW--"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.utils import to_categorical\n",
        "# Assuming 'y_train' and 'y_test' are the original string labels\n",
        "label_mapping = {\n",
        "    'Normal': 0,\n",
        "    'DDoS_UDP': 1,\n",
        "    'DDoS_ICMP': 2,\n",
        "    'DDoS_TCP': 3,\n",
        "    'Password': 4,\n",
        "    'DDoS_HTTP': 5,\n",
        "    'Vulnerability_scanner': 6,\n",
        "    'SQL_injection': 7 ,\n",
        "    'Backdoor': 8 ,\n",
        "    'Uploading':  9,\n",
        "    'Port_Scanning': 10,\n",
        "    'XSS': 11  ,\n",
        "    'Ransomware': 12,\n",
        "    'MITM' :  13 ,\n",
        "    'OS_Fingerprinting':  14\n",
        "    # Add other classes and their corresponding integer values here\n",
        "}\n",
        "\n",
        "# Convert string labels to integer labels\n",
        "y_train = [label_mapping[label] for label in y_train]\n",
        "y_test = [label_mapping[label] for label in y_test]\n",
        "\n"
      ],
      "metadata": {
        "id": "-PVPF5fXH6pH"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, batch_size=128, epochs=10, validation_split=0.1)\n"
      ],
      "metadata": {
        "id": "CV-3Y2uvHaEw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the test data\n",
        "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}\")"
      ],
      "metadata": {
        "id": "EZK0PREZHb_U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, Dropout, MaxPooling1D, Flatten, Dense\n",
        "\n",
        "# Define the batch size\n",
        "batch_size = 64\n",
        "\n",
        "# Define a generator function for data batching\n",
        "def data_generator(X, y, batch_size):\n",
        "    num_samples = X.shape[0]\n",
        "    indices = np.arange(num_samples)\n",
        "    np.random.shuffle(indices)\n",
        "    while True:\n",
        "        for start_idx in range(0, num_samples, batch_size):\n",
        "            end_idx = min(start_idx + batch_size, num_samples)\n",
        "            excerpt = indices[start_idx:end_idx]\n",
        "            yield X[excerpt], y[excerpt]\n",
        "\n",
        "# Create the TCN model\n",
        "model = Sequential()\n",
        "model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(19793248, 41)))\n",
        "model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(100, activation='relu'))\n",
        "model.add(Dense(15, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Create generator instances\n",
        "train_generator = data_generator(X_train, y_train, batch_size)\n",
        "test_generator = data_generator(X_test, y_test, batch_size)\n",
        "\n",
        "# Train the model using the generator\n",
        "steps_per_epoch = X_train.shape[0] // batch_size\n",
        "validation_steps = X_test.shape[0] // batch_size\n",
        "\n",
        "model.fit(train_generator, epochs=epochs, steps_per_epoch=steps_per_epoch,\n",
        "          validation_data=test_generator, validation_steps=validation_steps)\n"
      ],
      "metadata": {
        "id": "krdsp8TpC7RX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOckP_sYQcda",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2f06ea7-9e78-41b4-b11a-319278b5505e"
      },
      "source": [
        "df.Attack_type.value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Normal                   10066373\n",
              "DDoS_UDP                  3201626\n",
              "DDoS_ICMP                 2914354\n",
              "DDoS_TCP                  2020120\n",
              "Password                  1053385\n",
              "DDoS_HTTP                  229022\n",
              "Vulnerability_scanner      145869\n",
              "SQL_injection               51203\n",
              "Backdoor                    49724\n",
              "Uploading                   37634\n",
              "Port_Scanning               22564\n",
              "XSS                         15915\n",
              "Ransomware                  10925\n",
              "MITM                         2458\n",
              "OS_Fingerprinting            2002\n",
              "Name: Attack_type, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Data Transformation**"
      ],
      "metadata": {
        "id": "xQUllirvNrtd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert tabular data to images Procedures:\n",
        "\n",
        "Use quantile transform to transform the original data samples\n",
        "\n",
        "1  Élément de liste\n",
        "\n",
        "into the scale of [0,255], representing pixel values\n",
        "Generate images for each category (Normal, DoS, Fuzzy, Gear, RPM), each image consists of 27 data samples with 9 features. Thus, the size of each image is 993, length 9, width 9, and 3 color channels (RGB)."
      ],
      "metadata": {
        "id": "fVoNhyMgOB_q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dask dask-ml"
      ],
      "metadata": {
        "id": "0Az1A6ceaHx1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy==1.24.3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j8E1D1FSaW70",
        "outputId": "d83e807f-7800-4a84-c7aa-e181f55c340c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy==1.24.3 in /usr/local/lib/python3.10/dist-packages (1.24.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming you have your DataFrame 'df'\n",
        "\n",
        "# List of columns to drop\n",
        "columns_to_drop = ['mqtt.conack.flags', 'arp.opcode', 'udp.port', 'udp.time_delta',\n",
        "                   'dns.qry.name.len', 'dns.qry.qu', 'dns.qry.type', 'dns.retransmission',\n",
        "                   'dns.retransmit_request', 'dns.retransmit_request_in', 'icmp.unused',\n",
        "                   'mqtt.conflag.cleansess', 'mqtt.conflags', 'icmp.transmit_timestamp',\n",
        "                   'mqtt.msg_decoded_as', 'mqtt.msg', 'mqtt.proto_len', 'mqtt.protoname',\n",
        "                   'mqtt.topic', 'mqtt.topic_len', 'mbtcp.trans_id', 'mbtcp.unit_id']\n",
        "\n",
        "# Drop the specified columns\n",
        "df = df.drop(columns=columns_to_drop)\n"
      ],
      "metadata": {
        "id": "1Zpw6kF6hqlJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Initialize the MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Define a function to apply scaling to a partition\n",
        "def scale_partition(partition):\n",
        "    for col in partition.columns:\n",
        "        # Skip the label column\n",
        "        if col == 'Attack_type':\n",
        "            continue\n",
        "        # Apply scaling to non-numeric columns\n",
        "        if partition[col].dtype == 'int64':  # Assuming the encoded data type is int64\n",
        "            partition[col] = scaler.fit_transform(partition[col].values.reshape(-1, 1))\n",
        "    return partition\n",
        "\n",
        "# Apply scaling to each partition in the DataFrame\n",
        "scaled_df = df.map_partitions(scale_partition)\n",
        "\n",
        "# Compute the scaled DataFrame\n",
        "scaled_df = scaled_df.compute()\n",
        "\n",
        "# Print the scaled DataFrame\n",
        "print(scaled_df.head())"
      ],
      "metadata": {
        "id": "Ep3uvau6nUb1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.preprocessing import MinMaxScaler,QuantileTransformer\n",
        "\n",
        "# Initialize the MinMaxScaler\n",
        "scaler = QuantileTransformer()\n",
        "\n",
        "# Define a function to apply scaling to a partition\n",
        "def scale_partition(partition):\n",
        "    for col in partition.columns:\n",
        "        # Skip the label column\n",
        "        if col == 'Attack_type':\n",
        "            continue\n",
        "        # Apply scaling to non-numeric columns\n",
        "        if partition[col].dtype == 'int64':  # Assuming the encoded data type is int64\n",
        "            partition[col] = scaler.fit_transform(partition[col].values.reshape(-1, 1))\n",
        "    return partition\n",
        "\n",
        "# Apply scaling to each partition in the DataFrame\n",
        "scaled_df = df.map_partitions(scale_partition)\n",
        "\n",
        "# Compute the scaled DataFrame\n",
        "scaled_df = scaled_df.compute()\n",
        "\n",
        "# Print the scaled DataFrame\n",
        "print(scaled_df.head())"
      ],
      "metadata": {
        "id": "ouaMDIffi3zZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xalsFDNyQp66"
      },
      "source": [
        "df.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Define the file path\n",
        "file_path = '/content/drive/MyDrive/dl_final.csv'\n",
        "\n",
        "df = pd.read_csv(file_path,low_memory= False)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "vF7WGcKqrtNl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "tM5svMXnLpRk",
        "outputId": "dcb702e2-082e-4c18-da1a-cd23f3e5849b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(19793248, 41)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Replace 'your_data.csv' with the filename of your dataset\n",
        "# Assuming you already have your data in a DataFrame named 'df'\n",
        "# If you have not loaded your data into 'df', replace this with the appropriate code\n",
        "\n",
        "# Extract features and labels\n",
        "X = df.drop(columns=['Attack_type']).values\n",
        "y = df['Attack_type'].values\n",
        "\n",
        "# Output directory path\n",
        "output_dir = '/content/drive/MyDrive/DataSplits/'\n",
        "\n",
        "# Create the output directory if it doesn't exist\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Split the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "UQrfB8PDMId-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Output directory path\n",
        "output_dir = '/content/drive/MyDrive/DataSplits/'"
      ],
      "metadata": {
        "id": "SNjvHcfMSO3g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to save data to CSV file\n",
        "def save_to_csv(X, y, filename):\n",
        "    data = pd.DataFrame(data=np.column_stack((X, y)), columns=df.columns)\n",
        "    data.to_csv(filename, index=False)"
      ],
      "metadata": {
        "id": "aquQw9P_SV-Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save train data to CSV incrementally\n",
        "train_file = os.path.join(output_dir, 'train_data.csv')\n",
        "\n",
        "# Chunk size for incremental saving\n",
        "chunk_size = 100000\n",
        "\n",
        "# Save data in chunks\n",
        "num_samples = X_train.shape[0]\n",
        "for i in range(0, num_samples, chunk_size):\n",
        "    start_idx = i\n",
        "    end_idx = min(i + chunk_size, num_samples)\n",
        "\n",
        "    train_chunk = pd.DataFrame(data=np.column_stack((X_train[start_idx:end_idx], y_train[start_idx:end_idx])), columns=df.columns)\n",
        "\n",
        "    if i == 0:\n",
        "        # For the first chunk, include the header (column names)\n",
        "        train_chunk.to_csv(train_file, index=False, mode='a', header=True)\n",
        "    else:\n",
        "        # For subsequent chunks, exclude the header to avoid writing duplicate column names\n",
        "        train_chunk.to_csv(train_file, index=False, mode='a', header=False)\n",
        "\n",
        "    print(f\"Chunk {i//chunk_size + 1} saved.\")\n",
        "\n",
        "print(\"Train data saved incrementally.\")"
      ],
      "metadata": {
        "id": "AnhW3nNjSfHf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save test data to CSV incrementally\n",
        "test_file = os.path.join(output_dir, 'test_data.csv')\n",
        "\n",
        "# Chunk size for incremental saving\n",
        "chunk_size = 100000\n",
        "\n",
        "# Save data in chunks\n",
        "num_samples = X_test.shape[0]\n",
        "for i in range(0, num_samples, chunk_size):\n",
        "    start_idx = i\n",
        "    end_idx = min(i + chunk_size, num_samples)\n",
        "\n",
        "    test_chunk = pd.DataFrame(data=np.column_stack((X_test[start_idx:end_idx], y_test[start_idx:end_idx])), columns=df.columns)\n",
        "\n",
        "    if i == 0:\n",
        "        # For the first chunk, include the header (column names)\n",
        "        test_chunk.to_csv(test_file, index=False, mode='a', header=True)\n",
        "    else:\n",
        "        # For subsequent chunks, exclude the header to avoid writing duplicate column names\n",
        "        test_chunk.to_csv(test_file, index=False, mode='a', header=False)\n",
        "\n",
        "    print(f\"Chunk {i//chunk_size + 1} saved.\")\n",
        "\n",
        "print(\"Test data saved incrementally.\")"
      ],
      "metadata": {
        "id": "65iwBXTuPFI-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Just printing out the dimentions to verify the data\n",
        "print(X_train.shape, y_train.shape)\n",
        "print(X_test.shape, y_test.shape)"
      ],
      "metadata": {
        "id": "OPAgYCFgZffi",
        "outputId": "b7d5cf85-aa64-4c1f-f269-a455e1caf31d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(15834598, 40) (15834598,)\n",
            "(3958650, 40) (3958650,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the model\n",
        "\n",
        "# The model architecture type is sequential hence that is used\n",
        "model = Sequential()\n",
        "\n",
        "# We are using 4 convolution layers for feature extraction\n",
        "model.add(Conv1D(filters=512, kernel_size=32, padding='same', kernel_initializer='normal', activation='relu', input_shape=(19793248,41)))\n",
        "model.add(Conv1D(filters=512, kernel_size=32, padding='same', kernel_initializer='normal', activation='relu'))\n",
        "model.add(Dropout(0.2)) # This is the dropout layer. It's main function is to inactivate 20% of neurons in order to prevent overfitting\n",
        "model.add(Conv1D(filters=256, kernel_size=32, padding='same', kernel_initializer='normal', activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Conv1D(filters=256, kernel_size=32, padding='same', kernel_initializer='normal', activation='relu'))\n",
        "model.add(MaxPool1D(pool_size=128)) # We use MaxPooling with a filter size of 128. This also contributes to generalization\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "# The prevous step gices an output of multi dimentional data, which cannot be fead directly into the feed forward neural network. Hence, the model is flattened\n",
        "model.add(Flatten())\n",
        "# One hidden layer of 128 neurons have been used in order to have better classification results\n",
        "model.add(Dense(units=128, kernel_initializer='normal', activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "# The final neuron HAS to be 1 in number and cannot be more than that. This is because this is a binary classification problem and only 1 neuron is enough to denote the class '1' or '0'\n",
        "model.add(Dense(units=15, activation='softmax'))\n",
        "\n",
        "# Print the summary of the model\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "-dguHObld6-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "kLGT0-pIcAJc",
        "outputId": "8978c0b6-330b-45d7-c0ee-a8f2b238f44e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(19793248, 41)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Conv1D(filters=64, kernel_size=41, activation='relu', input_shape=(19793248,41)))\n",
        "model.add(Conv1D(filters=64, kernel_size=41, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(100, activation='relu'))\n",
        "model.add(Dense(15, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "mKb6ZlkZZmLO",
        "outputId": "95e8321d-5638-41c7-d173-705527a6be7b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-93b5b8fe0afe>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mConv1D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m41\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m19793248\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m41\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mConv1D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m41\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMaxPooling1D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpool_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Sequential' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Just printing out the dimentions to verify the data\n",
        "print(X_train.shape, y_train.shape)\n",
        "print(X_test.shape, y_test.shape)"
      ],
      "metadata": {
        "id": "ae1CinMUIgar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Generate images for each class**"
      ],
      "metadata": {
        "id": "VRknAZfvQ_eZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "0YXuJS5paUkB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torch==1.9.1"
      ],
      "metadata": {
        "id": "jQ0pV99BHMtg",
        "outputId": "a5b4eb87-9d90-433a-986f-382a252ba0ab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement torch==1.9.1 (from versions: 1.11.0, 1.12.0, 1.12.1, 1.13.0, 1.13.1, 2.0.0, 2.0.1)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for torch==1.9.1\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip -q install git+https://github.com/alok-ai-lab/pyDeepInsight.git#egg=pyDeepInsight"
      ],
      "metadata": {
        "id": "yoOIi_9PDYYK",
        "outputId": "71b38aec-7b7b-4324-b7cb-2c4662187c38",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install tensorflow\n",
        "! pip install keras\n",
        "! pip install numpy\n"
      ],
      "metadata": {
        "id": "Wg3BoZuA_9-L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4Y6pcf_RD-Dy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "import os\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import keras\n",
        "from keras import backend as K\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Input, Dense, Conv1D, Dropout, MaxPool1D, Flatten\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "LTH468vnAepK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This is not a mandatory step\n",
        "# This is used to set the figure size so that the plots appear to be of that size\n",
        "# If this parameter is not set, then the default plot size is used\n",
        "plt.rcParams['figure.figsize'] = 30, 15"
      ],
      "metadata": {
        "id": "cA2KL66g_wrV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This step is actually supposed to be used to randomly split dataset into training and testing set as well as to shuffle them\n",
        "# However, here I am using it just to shuffle the dataset and therefore the test_size is equated to 0\n",
        "X_train, _, y_train, _ = train_test_split(X_train, y_train, test_size=0, random_state=0)"
      ],
      "metadata": {
        "id": "htF5gRdREK8L"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}